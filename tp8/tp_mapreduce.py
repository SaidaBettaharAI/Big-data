# -*- coding: utf-8 -*-
"""TP mapreduce.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R2hfwSByLw5LQuKUf0xZ0CcgZwbGOe62
"""

from collections import defaultdict
import re
# Mapper

def mapper(line):

    words = re.findall(r'\b\w+\b', line.lower())
    return [(word, 1) for word in words]

# Shuffle / Group Phase
def group_pairs(pairs):
    grouped = defaultdict(list)
    for key, value in pairs:
        grouped[key].append(value)
    return grouped
# Reducer
def reducer_sum(grouped):
    return {key: sum(values) for key, values in grouped.items()}
# Main MapReduce
pairs = []

with open("data.txt", "r") as f:
    for line in f:
        pairs.extend(mapper(line))

grouped = group_pairs(pairs)
result = reducer_sum(grouped)
# Display results
print(" Word Count Results")
for word, count in sorted(result.items(), key=lambda x: -x[1]):
    print(f"{word}: {count}")

from collections import defaultdict

file_path = "sales.txt"

# Mapper Region
def mapper_region(row):
    qty = float(row['Quantity'])
    price = float(row['Price'])
    region = row['Region']
    revenue = qty * price
    return [(region, revenue)]
# Mapper Product
def mapper_product(row):
    qty = float(row['Quantity'])
    price = float(row['Price'])
    product = row['Product']
    revenue = qty * price
    return [(product, revenue)]

# Group / Shuffle
def group_pairs(pairs):
    grouped = defaultdict(list)
    for k,v in pairs:
        grouped[k].append(v)
    return grouped

# Reducer
def reducer_sum(grouped):
    return {k: sum(v) for k,v in grouped.items()}

rows = []
with open(file_path, "r") as f:
    for line in f:
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        parts = line.split(",")

        if len(parts) != 4:
            continue

        try:
            qty = float(parts[1])
            price = float(parts[2])
        except:
            continue
        row = {'Product': parts[0], 'Quantity': parts[1], 'Price': parts[2], 'Region': parts[3]}
        rows.append(row)

# MapReduce - Revenue per Region
pairs_region = []
for row in rows:
    pairs_region.extend(mapper_region(row))

grouped_region = group_pairs(pairs_region)
result_region = reducer_sum(grouped_region)

print(" Total Revenue per Region")
for region, total in sorted(result_region.items()):
    print(f"{region} → {total:.2f}")

# Bonus - Revenue per Product
pairs_product = []
for row in rows:
    pairs_product.extend(mapper_product(row))

grouped_product = group_pairs(pairs_product)
result_product = reducer_sum(grouped_product)

print("\n Total Revenue per Product")
for product, total in sorted(result_product.items()):
    print(f"{product} → {total:.2f}")

from collections import defaultdict

logfile_path = "weblogs.txt"
# Mapper: Status Code
def mapper_status(line):
    line = line.strip()
    if not line or line.startswith("#"):
        return []
    parts = line.split(",")
    if len(parts) < 6:
        return []
    status = parts[5].strip()
    return [(status, 1)]
# Mapper: URL
def mapper_url(line):
    line = line.strip()
    if not line or line.startswith("#"):
        return []
    parts = line.split(",")
    if len(parts) < 5:
        return []
    url = parts[4].strip()
    return [(url, 1)]

# Mapper: Status → ResponseSize
def mapper_status_size(line):
    line = line.strip()
    if not line or line.startswith("#"):
        return []
    parts = line.split(",")
    if len(parts) < 7:
        return []
    status = parts[5].strip()
    try:
        size = int(parts[6].strip())
    except:
        size = 0
    return [(status, size)]
# Group / Shuffle
def group_pairs(pairs):
    grouped = defaultdict(list)
    for k, v in pairs:
        grouped[k].append(v)
    return grouped

# Reducer: sum values
def reducer_sum(grouped):
    return {k: sum(v) for k, v in grouped.items()}

with open(logfile_path, "r", encoding="utf-8") as f:
    lines = f.readlines()

# 1. Total Requests per Status Code
pairs_status = []
for line in lines:
    pairs_status.extend(mapper_status(line))

grouped_status = group_pairs(pairs_status)
result_status = reducer_sum(grouped_status)

print(" Total Requests per Status Code")
for status, count in sorted(result_status.items()):
    print(f"HTTP {status}: {count} requests")

# 2. Total Requests per URL (Bonus)
pairs_url = []
for line in lines:
    pairs_url.extend(mapper_url(line))

grouped_url = group_pairs(pairs_url)
result_url = reducer_sum(grouped_url)

print("\n Total Requests per URL")
for url, count in sorted(result_url.items()):
    print(f"{url}: {count} requests")

# 3. Total Response Size per Status Code (Bonus)
pairs_status_size = []
for line in lines:
    pairs_status_size.extend(mapper_status_size(line))

grouped_status_size = group_pairs(pairs_status_size)
result_status_size = reducer_sum(grouped_status_size)

print("\n Total Response Size per Status Code")
for status, total_size in sorted(result_status_size.items()):
    print(f"HTTP {status}: {total_size} bytes")

error_status = {k:v for k,v in result_status.items() if k != "200"}
print("\n Errors Only (status != 200)")
for status, count in sorted(error_status.items()):
    print(f"HTTP {status}: {count} requests")

from collections import defaultdict

logfile_path = "weblogs.txt"
# Mapper Functions
def mapper_url(line):
    parts = line.strip().split(",")
    if len(parts) < 5:
        return []
    url = parts[4].strip()
    return [(url, 1)] if url else []

# 2) Mapper لحجم الاستجابة لكل Status
def mapper_status_size(line):
    parts = line.strip().split(",")
    if len(parts) < 7:
        return []
    status = parts[5].strip()
    try:
        size = int(parts[6].strip())
    except:
        size = 0
    return [(status, size)]

# 3) Mapper للأخطاء فقط (تجاهل 200)
def mapper_errors(line):
    parts = line.strip().split(",")
    if len(parts) < 6:
        return []
    status = parts[5].strip()
    if status == "200":
        return []
    return [(status, 1)]

# Shuffle / Group Function
def group_pairs(pairs):
    grouped = defaultdict(list)
    for k, v in pairs:
        grouped[k].append(v)
    return grouped

# Reducers
def reducer_count(grouped):
    return {k: sum(vs) for k, vs in grouped.items()}

def reducer_sum(grouped):
    return {k: sum(vs) for k, vs in grouped.items()}


with open(logfile_path, "r", encoding="utf-8") as f:
    lines = [line for line in f if not line.startswith("#")]
# 1) عدد الطلبات لكل URL
pairs_url = []
for line in lines:
    pairs_url.extend(mapper_url(line))
grouped_url = group_pairs(pairs_url)
result_url = reducer_count(grouped_url)

print("Requests per URL")
for url, count in result_url.items():
    print(f"{url}: {count} requests")


pairs_status_size = []
for line in lines:
    pairs_status_size.extend(mapper_status_size(line))
grouped_status_size = group_pairs(pairs_status_size)
result_status_size = reducer_sum(grouped_status_size)

print("\n Total Response Size per Status")
for status, total_size in result_status_size.items():
    print(f"HTTP {status}: {total_size} bytes")


pairs_errors = []
for line in lines:
    pairs_errors.extend(mapper_errors(line))
grouped_errors = group_pairs(pairs_errors)
result_errors = reducer_count(grouped_errors)

print("\n Error Requests (excluding 200)")
for status, count in result_errors.items():
    print(f"HTTP {status}: {count} requests")